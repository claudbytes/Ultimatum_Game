---
title: 'Homework 3: Logistic Regression'
author: "L4 Adv Stats Team"
date: "13 November 2017"
output: html_document
---

```{r setup, include=FALSE, message = FALSE}
## PLEASE DO NOT MODIFY ANYTHING IN THIS SETUP CHUNK! 

knitr::opts_chunk$set(echo = TRUE)

library("tidyverse")

subj <- readRDS("subj.rds")
trials <- readRDS("trials.rds")
fillers <- readRDS("fillers.rds")
```

# Instructions

Leave everything above this section in the file. This RMarkdown file should be in your working directory as an .Rmd file with the data files you'll be working with (binary R files, with the `.rds` extension).

You should then set your working directory for the console to point to this same location (from the pulldown menu, select Session | Set Working Directory...). 

*NOTE*: in all of the problems below, you will need to replace `NULL` with a value or some code that computes a value, vector, or table.  When altering code inside the code blocks, please *do not* re-order or rename the code blocks (T1, T2, ... etc.).  Also, do NOT rename any of the variables you have been provided.  If you do either of these things, this will impact your grade!

It's also recommended that you "Knit" a report to be able to see what you've accomplished and spot potential errors. BUT DO NOT SUBMIT THE HTML version as homework: submit the .Rmd file with your answers through the link provided on the Moodle page.  Please make sure the RMarkdown file that you submit compiles (i.e., knits to HTML) without any errors.  This is *very important*: producing a reproducible script is a *key* part of the assessment!

# Background

You will be looking at a dataset investigating issues in language production relating to the production of so-called “filled pauses” among speakers of Dutch and American English. This is real data collected by my lab in the U.S.A. and a colleague’s lab in the Netherlands around 2009. Each student will get a randomly selected subset of 18 Dutch and 18 American participants from the original dataset (which had 20 from each language group).

In this experiment, speakers were asked to describe unusual complicated shapes, like those seen in the set below:

![](pics.png)

In each trial of the experiment, participants saw a pair of pictures on the screen from the image set, and had to name one of the two pictures so that a listener could select is using a computer mouse. Because these pictures were hard to describe, speakers would often hesitate and produce so-called “fillers” (“um” or “uh”) preceding their descriptions. For instance, a speaker might say:

> 
> [480 ms silence] um.. [1022 ms silence] it looks like three snails stacked up
>

Note that there are pauses (measured in milliseconds) preceding and following the filler (in the example, ’um’). Your task in this assignment is to investigate the properties that cause speakers to produce fillers, as well as the factors that govern the choice of filler type (’um’ versus ’uh’).

Each picture was named three times (pictures get easier to describe each time, so the prediction would be that fillers get less likely).

There are three RDS files, which have been loaded for you in the setup block above, using `readRDS()`  There are also some functions that you might find useful that have been defined for you in the setup block.

Note: Although this is multilevel data (and thus, observations are not independent), we will be ignoring this multilevel structure for the purposes of the analysis. In other words, for the purposes of this assignment, please ignore the dependencies introduced by repeated sampling over participants (`subj_id`) and pictures (`pic_id`) and treat each observation as independent. Although this is technically incorrect, we haven’t yet learned how to handle such dependencies.

The data are stored in three separate files.  We will be mainly working with `fillers`, but there are some extra tasks at the end that require the two additional tables.

* `subj.rds`
Information about participants

    column   |description
    ---------|-----------
    subj_id  | unique number identifying each subject
    nat      | nationality (Amer= American or Dutch)


* `trials.rds`
Information about individual trials 

    column   |description
    ---------|-----------
    subj_id  | unique number identifying each subject
    trial_id | unique number identifying each trial
    pic_id   | unique number identifying each picture
    prev_men | number of times picture was previously named by subject
    filler   | identity of filler that was produced: no = no filler; uh,um; ot= other
    onset    | latency onset of first (non-filler) word in the description (milliseconds)
    
    
* `fillers.rds`
Information about individual fillers that were produced

    column   |description
    ---------|-----------
    trial_id | unique number identifying each trial
    subj_id  | unique number identifying each subject
    sp_pre   | length of silent pause preceding the filler (milliseconds)
    sp_pre   | length of silent pause following the filler (milliseconds)



## Task 1

Before proceeding, remove all fillers of type 'ot' (other) from the dataset.  Store the resulting table in the variable `tclean`.  Use this as your source dataset in the following blocks.

```{r T1}
## get rid of all fillers of type 'ot'
tclean <- trials %>%
  filter(filler != "ot")
```


## Task 2

How many fillers of each type are in your dataset?  The result `n_types` should be a table with exactly two columns named `filler` and `n` of types `character` and `integer`, respectively.

```{r T2}
n_types <- count(tclean, filler)
```

## Task 3

We are first going to attempt to model the probability of a filler of any type (um or uh) as a function of the length of the pause preceding the first content word, as given by the variable `onset`.

The first thing that we should do is express onset in seconds rather than milliseconds.  Using a tidyverse function, add a new column to `tclean` called `onset_s` which expresses `onset` in seconds, rather than milliseconds.  Store the resulting table in `tclean2`.

```{r T3}
tclean2 <- tclean %>%
  mutate(onset_s = onset / 1000)
```

## Task 4

Let's have a look at the distribution of `onset_s` in the dataset `tclean2`.  Write code to create the graph below.  (NOTE: your histogram may differ slightly, as each student receives different data.)

```{r T4}
## TODO: your ggplot code here
ggplot(tclean2,
       aes(onset_s)) + geom_histogram()
```

![](onset_dist.png)

## Task 5

OK, that distribution does not look ideal.  Create a new variable in the `tclean2` dataset called `onset_slog` that is the log of `onset_s`, and store the resulting table in `tclean3`.  Plot the distribution of `onset_slog`.

```{r T5}
tclean3 <- tclean2 %>%
  mutate(onset_slog = log(onset_s))

## TODO: your ggplot code here
ggplot(tclean3, aes(onset_slog)) + geom_histogram()
```

That looks much better!  We will use `onset_slog` in our modeling.

## Task 6

The variable containing the behavior you want to model is `filler`, which has three levels.  But to fit a logistic regression model, you first need a *dichotomous* response variable---a variable that has two categories.  Recode `filler` from `tclean3` into a new variable `anyfill` of type integer or numeric (*not* factor), which codes 'um' or 'uh' as `1` and 'no' as `0`.  The resulting table should be stored in the variable `tclean_af`.

*HINT*: `recode()`, `if_else()`, `case_when()`

```{r T6}
tclean_af <- tclean3 %>%
  mutate(anyfill = if_else(filler == "no", 0L, 1L))
```

## Task 7

Categorical data can be difficult to visualize, because they are fundamentally just 1s and 0s, but it is important to nonetheless get some idea about the pattern you are trying to model before you go any further.

The main problem with categorical variable is that for each `x` observation (`onset`), you have a single `0` or `1` outcome variable.  It would be nice to plot proportions of "successes"" (e.g., producing a filler) but we can't do this for individual `x` values; however, we can do it for a *range* of `x` values. 

One common strategy to *bin* your predictor variable, which means breaking it into ranges and calculating an aggregate value for the range.

To understand how to bin data, study the code below, which bins up data from an example table and plots it.

```{r binning}
## THIS CODE BLOCK GIVES AN EXAMPLE; IT IS NOT PART OF THE ASSESSMENT
example_data <- tibble(x = 1:1000, 
                       y = sample(0:1, 1000, TRUE))

binned <- example_data %>%
  mutate(bin = floor((x + 50) / 100) * 100)

bin_sum <- binned %>%
  group_by(bin) %>%
  summarise(p = mean(y))

ggplot(bin_sum, aes(bin, p)) + 
  geom_line() + 
  geom_point() +
  coord_cartesian(ylim = c(0, 1)) +
  labs(y = "probability")
```

In the code chunk below, we will be working with `onset` (which is in milliseconds), not `onset_slog`.  The reason is that the transformed variable `onset_slog` has better properties for modeling the data, but is worse for understanding the data, so it's better to view the data in terms of `onset`.  (In fact, it's best to look at it both ways...)

Based on the example above, use the `tclean_af` table, bin up `onset` into 500ms bins (call the binned variable `onset_bin`), and calculate the probability of a filler (`p`) for each bin.  The resulting table should contain `onset_bin` and `p`, and should be named `tclean_bin`.  Make a plot like the one above, showing `p` as a function of `onset_bin`.

```{r T7}
tclean_bin <- tclean_af %>%
  mutate(onset_bin = floor((onset + 250) / 500) * 500) %>%
  group_by(onset_bin) %>%
  summarise(p = mean(anyfill))

ggplot(tclean_bin, aes(onset_bin, p)) +
  geom_line() +
  geom_point()
```

## Task 8

Fit a logistic regression on the dataset `tclean_af` with `anyfill` as the response measure and `onset_slog` as the predictor.  The resulting model *must* be stored in `mod`.

```{r T8}
mod <- glm(anyfill ~ onset_slog, binomial, tclean_af)

summary(mod)  # uncomment this line once you've defined mod
```

## Task 9

Next we want to visualise the model predictions; in particular, we want to see the *probability of a filler* as a function of the delay (`onset_slog`).

As a first step, here is an inverse logit function `logit_inv` that takes a value of `x` (i.e., `onset_slog`) and the model coefficients, and returns a probability.  There are some example function calls below.  Play around with the function until you understand what it is doing.

```{r logitinv}
## YOU DON'T NEED TO CHANGE ANYTHING IN THIS CODE BLOCK;
## IT IS NOT PART OF THE ASSESSMENT

## inverse logit function
logit_inv <- function(x, int, slp) {
  1 / (1 + exp(-(int + slp * x)))
}

x <- seq(-.7, 3, .1)  ## just generate some x values
coef(mod) # gives you the parameter estimates from your model (intercept and slope)

logit_inv(x, int = 0, slp = 5)
logit_inv(x, int = coef(mod)[1], slp = coef(mod)[2])
```

Now create a plot of the predicted probability over the range of `onset_slog` observed in your data.  It should have a sigmoidal shape.  You can use the shiny app to check.

We will use `ggplot::stat_function()`.  Search for examples of this (`?stat_function`) to learn more about it.

Below is example code that you should modify so that it prints the probability of a filler (as estimated from your model) as a function of `onset_slog`, over the full range of `onset_slog` in your data.  For the `int` and `slp` arguments use code that computes the parameter estimates from your model (don't just type the number).

```{r T9}
## TODO add any additional code you need here.
dat <- tibble(x = c(-1, 4))

ggplot(dat, aes(x)) + 
  stat_function(fun = logit_inv, 
                geom = "path",
                args = list(int = coef(mod)[1], 
                            slp = coef(mod)[2]))
```

## Task 10

According to your model, how long does the hesitation need to be (in log units) for the probability of a filler to be .5?  Use *code* to generate the value from your model; don't just type the numbers.

```{r T41}
## TODO: add any code that you need here

pause_len <- -coef(mod)[1] / coef(mod)[2]
```

# Extra tasks (not assessed)

If you would like a further challenge, have a go at these two extra tasks.  They are not assessed, so you don't have to complete them; but you will be given solutions that you can learn from.

## Extra task 1

Estimate a model predicting the log odds of any filler (uh or um) based on nationality.  Interpret your results.

```{r ET1}
## NOT ASSESSED
dnat <- tclean_af %>%
  inner_join(subj, "subj_id") %>%
  mutate(nt = ifelse(nat == "Amer", .5, -.5))

mod_nat <- glm(anyfill ~ nt, binomial, dnat)
summary(mod_nat)

```

Odds of producing a filler were `r exp(coef(mod_nat)[2]) %>% round(2)` times higher for American participants

## Extra task 2

For this question, you will first need to subset your data to only include those trials in which speakers produced an um or an uh.
Clark and Fox Tree (2002) found evidence that the choice of um versus uh seemed to depend upon the length of the pause following the filler (`sp_post`), but not the length of the pause preceding it (`sp_pre`). Can you replicate their finding? Use logistic regression. Explain and interpret your results.

```{r ET2}
## NOT ASSESSED

uhum <- trials %>%
  filter(filler %in% c("uh", "um")) %>%
  mutate(onset_s = (onset / 1000),
         oc = onset_s - mean(onset_s), # centered version
         ftype = ifelse(filler == "um", 1L, 0L))

mod_uhum <- glm(ftype ~ oc, binomial, uhum)

summary(mod_uhum)
```

Probability that the filler produced is *um* instead of *uh* increases by a factor of `r exp(coef(mod_uhum)[2]) %>% round(2)` for each additional second of delay
